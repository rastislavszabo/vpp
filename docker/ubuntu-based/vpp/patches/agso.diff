From f90b59b749d86676616a9e760fe6f316fb4e245e Mon Sep 17 00:00:00 2001
From: Andrew Yourtchenko <ayourtch@gmail.com>
Date: Fri, 12 Oct 2018 16:09:22 +0200
Subject: [PATCH] tap gso: experimental support

This commit adds a debug CLI which per-interface enables GSO:

"set tap gso {<interface> | sw_if_index <sw_idx>} <enable|disable>"

It does the necessary syscalls to enable the GSO
and checksum offload support on the kernel side and sets two flags
on the interface: virtio-specific virtio_if_t.gso_enabled,
and vnet_hw_interface_t.flags & VNET_HW_INTERFACE_FLAG_SUPPORTS_GSO.

The first one, if enabled, triggers the marking of the GSO-encapsulated
packets on ingress with VNET_BUFFER_F_GSO flag, and
setting vnet_buffer2(b)->gso_size to the desired L4 payload size.

VNET_HW_INTERFACE_FLAG_SUPPORTS_GSO determines the egress packet
processing in interface-output for such packets:

When the flag is set, they are sent out almost as usual (just taking
care to set the vnet header for virtio).

When the flag is cleared (the case for most interfaces),
the egress path performs the re-segmentation such that
the L4 payload of the transmitted packets equals gso_size.

This commit lacks IPv6 extension header traversal support of any kind -
the L4 payload is assumed to follow the IPv6 header.

Change-Id: Ifd562db89adcc2208094b3d1032cee8c307aaef9
Signed-off-by: Andrew Yourtchenko <ayourtch@gmail.com>
---

diff --git a/src/vnet/buffer.h b/src/vnet/buffer.h
index 8071081..68477db 100644
--- a/src/vnet/buffer.h
+++ b/src/vnet/buffer.h
@@ -64,7 +64,8 @@
   _(16, L4_HDR_OFFSET_VALID, 0)				\
   _(17, FLOW_REPORT, "flow-report")			\
   _(18, IS_DVR, "dvr")                                  \
-  _(19, QOS_DATA_VALID, 0)
+  _(19, QOS_DATA_VALID, 0)                              \
+  _(20, GSO, "gso")                                     \
 
 #define VNET_BUFFER_FLAGS_VLAN_BITS \
   (VNET_BUFFER_F_VLAN_1_DEEP | VNET_BUFFER_F_VLAN_2_DEEP)
@@ -368,6 +369,20 @@
     u16 src_epg;
   } gbp;
 
+
+  /**
+   * The L4 payload size set on input on GSO enabled interfaces
+   * when we receive a GSO packet (a chain of 2K buffers with the first one
+   * having GSO bit set), and needs to persist all the way to the interface-output,
+   * in case the egress interface is not GSO-enabled - then we need to perform
+   * the segmentation, and use this value to cut the payload appropriately.
+   */
+  u16 gso_size;
+
+  /* The union below has a u64 alignment, so this space is unused */
+  u16 __unused1[1];
+  u32 __unused2[1];
+
   union
   {
     struct
@@ -382,7 +397,7 @@
       u64 pad[1];
       u64 pg_replay_timestamp;
     };
-    u32 unused[10];
+    u32 unused[8];
   };
 } vnet_buffer_opaque2_t;
 
diff --git a/src/vnet/devices/tap/cli.c b/src/vnet/devices/tap/cli.c
index 9d86159..8eb0e41 100644
--- a/src/vnet/devices/tap/cli.c
+++ b/src/vnet/devices/tap/cli.c
@@ -163,6 +163,59 @@
 /* *INDENT-ON* */
 
 static clib_error_t *
+tap_gso_command_fn (vlib_main_t * vm, unformat_input_t * input,
+		    vlib_cli_command_t * cmd)
+{
+  unformat_input_t _line_input, *line_input = &_line_input;
+  u32 sw_if_index = ~0;
+  vnet_main_t *vnm = vnet_get_main ();
+  int enable = 1;
+  int rv;
+
+  /* Get a line of input. */
+  if (!unformat_user (input, unformat_line_input, line_input))
+    return clib_error_return (0, "Missing <interface>");
+
+  while (unformat_check_input (line_input) != UNFORMAT_END_OF_INPUT)
+    {
+      if (unformat (line_input, "sw_if_index %d", &sw_if_index))
+	;
+      else if (unformat (line_input, "%U", unformat_vnet_sw_interface,
+			 vnm, &sw_if_index))
+	;
+      else if (unformat (line_input, "enable"))
+	enable = 1;
+      else if (unformat (line_input, "disable"))
+	enable = 0;
+      else
+	return clib_error_return (0, "unknown input `%U'",
+				  format_unformat_error, input);
+    }
+  unformat_free (line_input);
+
+  if (sw_if_index == ~0)
+    return clib_error_return (0,
+			      "please specify interface name or sw_if_index");
+
+  rv = tap_gso_enable_disable (vm, sw_if_index, enable);
+  if (rv == VNET_API_ERROR_INVALID_SW_IF_INDEX)
+    return clib_error_return (0, "not a tap interface");
+  else if (rv != 0)
+    return clib_error_return (0, "error on configuring GSO on tap interface");
+
+  return 0;
+}
+
+/* *INDENT-OFF* */
+VLIB_CLI_COMMAND (tap_gso__command, static) =
+{
+  .path = "set tap gso",
+  .short_help = "set tap gso {<interface> | sw_if_index <sw_idx>} <enable|disable>",
+  .function = tap_gso_command_fn,
+};
+/* *INDENT-ON* */
+
+static clib_error_t *
 tap_show_command_fn (vlib_main_t * vm, unformat_input_t * input,
 		     vlib_cli_command_t * cmd)
 {
diff --git a/src/vnet/devices/tap/tap.c b/src/vnet/devices/tap/tap.c
index d3ed2af..016e3e1 100644
--- a/src/vnet/devices/tap/tap.c
+++ b/src/vnet/devices/tap/tap.c
@@ -174,6 +174,8 @@
   unsigned int offload = 0;
   hdrsz = sizeof (struct virtio_net_hdr_v1);
   _IOCTL (vif->tap_fd, TUNSETOFFLOAD, offload);
+  vif->gso_enabled = 0;
+
   _IOCTL (vif->tap_fd, TUNSETVNETHDRSZ, &hdrsz);
   _IOCTL (vif->fd, VHOST_SET_OWNER, 0);
 
@@ -456,6 +458,46 @@
 }
 
 int
+tap_gso_enable_disable (vlib_main_t * vm, u32 sw_if_index, int enable_disable)
+{
+  vnet_main_t *vnm = vnet_get_main ();
+  virtio_main_t *mm = &virtio_main;
+  virtio_if_t *vif;
+  vnet_hw_interface_t *hw;
+  clib_error_t *err = 0;
+
+  hw = vnet_get_sup_hw_interface (vnm, sw_if_index);
+  if (hw == NULL || virtio_device_class.index != hw->dev_class_index)
+    return VNET_API_ERROR_INVALID_SW_IF_INDEX;
+
+  vif = pool_elt_at_index (mm->interfaces, hw->dev_instance);
+
+  const unsigned int gso_on =
+    TUN_F_CSUM | TUN_F_TSO4 | TUN_F_TSO6 | TUN_F_UFO;
+  const unsigned int gso_off = 0;
+  unsigned int offload = enable_disable ? gso_on : gso_off;
+  _IOCTL (vif->tap_fd, TUNSETOFFLOAD, offload);
+  vif->gso_enabled = enable_disable ? 1 : 0;
+  if (enable_disable)
+    {
+      hw->flags |= VNET_HW_INTERFACE_FLAG_SUPPORTS_GSO;
+    }
+  else
+    {
+      hw->flags &= ~VNET_HW_INTERFACE_FLAG_SUPPORTS_GSO;
+    }
+
+error:
+  if (err)
+    {
+      clib_warning ("Error %s gso on sw_if_index %d",
+		    enable_disable ? "enabling" : "disabling", sw_if_index);
+      return VNET_API_ERROR_SYSCALL_ERROR_3;
+    }
+  return 0;
+}
+
+int
 tap_dump_ifs (tap_interface_details_t ** out_tapids)
 {
   vnet_main_t *vnm = vnet_get_main ();
diff --git a/src/vnet/devices/tap/tap.h b/src/vnet/devices/tap/tap.h
index 7aebade..71c23a4 100644
--- a/src/vnet/devices/tap/tap.h
+++ b/src/vnet/devices/tap/tap.h
@@ -76,6 +76,8 @@
 
 void tap_create_if (vlib_main_t * vm, tap_create_if_args_t * args);
 int tap_delete_if (vlib_main_t * vm, u32 sw_if_index);
+int tap_gso_enable_disable (vlib_main_t * vm, u32 sw_if_index,
+			    int enable_disable);
 int tap_dump_ifs (tap_interface_details_t ** out_tapids);
 
 #endif /* _VNET_DEVICES_VIRTIO_TAP_H_ */
diff --git a/src/vnet/devices/virtio/device.c b/src/vnet/devices/virtio/device.c
index c7efe65..fbca3d3 100644
--- a/src/vnet/devices/virtio/device.c
+++ b/src/vnet/devices/virtio/device.c
@@ -133,6 +133,14 @@
   struct virtio_net_hdr_v1 *hdr = vlib_buffer_get_current (b) - hdr_sz;
 
   memset (hdr, 0, hdr_sz);
+  if (b->flags & VNET_BUFFER_F_GSO)
+    {
+      hdr->gso_type = VIRTIO_NET_HDR_GSO_TCPV4;
+      hdr->gso_size = vnet_buffer2 (b)->gso_size;
+      hdr->flags = VIRTIO_NET_HDR_F_NEEDS_CSUM;
+      hdr->csum_start = 0x22;
+      hdr->csum_offset = 0x10;
+    }
 
   if (PREDICT_TRUE ((b->flags & VLIB_BUFFER_NEXT_PRESENT) == 0))
     {
diff --git a/src/vnet/devices/virtio/node.c b/src/vnet/devices/virtio/node.c
index 339c48c..e150a0d 100644
--- a/src/vnet/devices/virtio/node.c
+++ b/src/vnet/devices/virtio/node.c
@@ -32,11 +32,12 @@
 #include <vnet/feature/feature.h>
 #include <vnet/ip/ip4_packet.h>
 #include <vnet/ip/ip6_packet.h>
+#include <vnet/udp/udp_packet.h>
 #include <vnet/devices/virtio/virtio.h>
 
 
 #define foreach_virtio_input_error \
-  _(UNKNOWN, "unknown")
+  _(UNKNOWN_GSO_TYPE, "unknown GSO type")
 
 typedef enum
 {
@@ -102,6 +103,7 @@
     {
       struct vring_desc *d = &vring->desc[next];;
       vlib_buffer_t *b = vlib_get_buffer (vm, vring->buffers[next]);
+      b->error = 0;
       d->addr = pointer_to_uword (vlib_buffer_get_current (b)) - hdr_sz;
       d->len = VLIB_BUFFER_DATA_SIZE + hdr_sz;
       d->flags = VRING_DESC_F_WRITE;
@@ -123,9 +125,78 @@
     }
 }
 
+static_always_inline void
+fill_gso_buffer_flags (vlib_buffer_t * b0, struct virtio_net_hdr_v1 *hdr)
+{
+  u8 l4_proto = 0;
+  if ((hdr->gso_type != VIRTIO_NET_HDR_GSO_TCPV4) &&
+      (hdr->flags & VIRTIO_NET_HDR_F_NEEDS_CSUM))
+
+    {
+      ethernet_header_t *eh = (ethernet_header_t *) b0->data;
+      u16 ethertype = clib_net_to_host_u16 (eh->type);
+      u16 l2hdr_sz = sizeof (ethernet_header_t);
+
+      vnet_buffer (b0)->l2_hdr_offset = 0;
+      vnet_buffer (b0)->l3_hdr_offset = l2hdr_sz;
+      if (PREDICT_TRUE (ethertype == ETHERNET_TYPE_IP4))
+	{
+	  ip4_header_t *ip4 = (ip4_header_t *) (b0->data + l2hdr_sz);
+	  vnet_buffer (b0)->l4_hdr_offset = l2hdr_sz + ip4_header_bytes (ip4);
+	  l4_proto = ip4->protocol;
+	  b0->flags |=
+	    (VNET_BUFFER_F_IS_IP4 | VNET_BUFFER_F_L2_HDR_OFFSET_VALID
+	     | VNET_BUFFER_F_L3_HDR_OFFSET_VALID |
+	     VNET_BUFFER_F_L4_HDR_OFFSET_VALID);
+	}
+      else if (PREDICT_TRUE (ethertype == ETHERNET_TYPE_IP6))
+	{
+	  ip6_header_t *ip6 = (ip6_header_t *) (b0->data + l2hdr_sz);
+	  vnet_buffer (b0)->l4_hdr_offset = l2hdr_sz + sizeof (ip6_header_t);	/* FIXME IPv6 EH traversal */
+	  l4_proto = ip6->protocol;
+	  b0->flags |=
+	    (VNET_BUFFER_F_IS_IP4 | VNET_BUFFER_F_L2_HDR_OFFSET_VALID
+	     | VNET_BUFFER_F_L3_HDR_OFFSET_VALID |
+	     VNET_BUFFER_F_L4_HDR_OFFSET_VALID);
+	}
+      if (l4_proto == IP_PROTOCOL_TCP)
+	{
+	  b0->flags |= VNET_BUFFER_F_OFFLOAD_TCP_CKSUM;
+	  tcp_header_t *tcp = (tcp_header_t *) (b0->data +
+						vnet_buffer
+						(b0)->l4_hdr_offset);
+	  tcp->checksum = 0;
+	}
+      else if (l4_proto == IP_PROTOCOL_UDP)
+	{
+	  b0->flags |= VNET_BUFFER_F_OFFLOAD_UDP_CKSUM;
+	  udp_header_t *udp = (udp_header_t *) (b0->data +
+						vnet_buffer
+						(b0)->l4_hdr_offset);
+	  udp->checksum = 0;
+
+	}
+    }
+
+  if (hdr->gso_type == VIRTIO_NET_HDR_GSO_TCPV4)
+    {
+      vnet_buffer2 (b0)->gso_size = hdr->gso_size;
+      b0->flags |= VNET_BUFFER_F_GSO;
+      b0->flags |= VNET_BUFFER_F_IS_IP4;
+    }
+  if (hdr->gso_type == VIRTIO_NET_HDR_GSO_TCPV6)
+    {
+      vnet_buffer2 (b0)->gso_size = hdr->gso_size;
+      b0->flags |= VNET_BUFFER_F_GSO;
+      b0->flags |= VNET_BUFFER_F_IS_IP6;
+    }
+}
+
+
 static_always_inline uword
 virtio_device_input_inline (vlib_main_t * vm, vlib_node_runtime_t * node,
-			    vlib_frame_t * frame, virtio_if_t * vif, u16 qid)
+			    vlib_frame_t * frame, virtio_if_t * vif, u16 qid,
+			    int gso_enabled)
 {
   vnet_main_t *vnm = vnet_get_main ();
   u32 thread_index = vm->thread_index;
@@ -165,6 +236,10 @@
 	  b0->current_length = len;
 	  b0->total_length_not_including_first_buffer = 0;
 	  b0->flags = VLIB_BUFFER_TOTAL_LENGTH_VALID;
+
+	  if (gso_enabled)
+	    fill_gso_buffer_flags (b0, hdr);
+
 	  vnet_buffer (b0)->sw_if_index[VLIB_RX] = vif->sw_if_index;
 	  vnet_buffer (b0)->sw_if_index[VLIB_TX] = (u32) ~ 0;
 
@@ -265,8 +340,12 @@
     mif = vec_elt_at_index (nm->interfaces, dq->dev_instance);
     if (mif->flags & VIRTIO_IF_FLAG_ADMIN_UP)
       {
-	n_rx += virtio_device_input_inline (vm, node, frame, mif,
-					    dq->queue_id);
+	if (mif->gso_enabled)
+	  n_rx += virtio_device_input_inline (vm, node, frame, mif,
+					      dq->queue_id, 1);
+	else
+	  n_rx += virtio_device_input_inline (vm, node, frame, mif,
+					      dq->queue_id, 0);
       }
   }
 
diff --git a/src/vnet/devices/virtio/virtio.h b/src/vnet/devices/virtio/virtio.h
index 5fc5216..068a349 100644
--- a/src/vnet/devices/virtio/virtio.h
+++ b/src/vnet/devices/virtio/virtio.h
@@ -115,6 +115,7 @@
   u8 host_ip4_prefix_len;
   ip6_address_t host_ip6_addr;
   u8 host_ip6_prefix_len;
+  int gso_enabled;
 
   int ifindex;
 } virtio_if_t;
diff --git a/src/vnet/interface.c b/src/vnet/interface.c
index 45a265c..285e32c 100644
--- a/src/vnet/interface.c
+++ b/src/vnet/interface.c
@@ -857,6 +857,7 @@
 	static char *e[] = {
 	  "interface is down",
 	  "interface is deleted",
+	  "no buffers to segment GSO",
 	};
 
 	r.n_errors = ARRAY_LEN (e);
@@ -1291,6 +1292,12 @@
       }
   }
 
+  /* init per-thread data */
+  vlib_thread_main_t *tm = vlib_get_thread_main ();
+  vec_validate_aligned (im->per_thread_data, tm->n_vlib_mains - 1,
+			CLIB_CACHE_LINE_BYTES);
+
+
   if ((error = vlib_call_init_function (vm, vnet_interface_cli_init)))
     return error;
 
diff --git a/src/vnet/interface.h b/src/vnet/interface.h
index 7ce6aaf..e9b567c 100644
--- a/src/vnet/interface.h
+++ b/src/vnet/interface.h
@@ -534,6 +534,9 @@
   /* tx checksum offload */
 #define VNET_HW_INTERFACE_FLAG_SUPPORTS_TX_L4_CKSUM_OFFLOAD (1 << 17)
 
+  /* GSO */
+#define VNET_HW_INTERFACE_FLAG_SUPPORTS_GSO (1 << 18)
+
   /* Hardware address as vector.  Zero (e.g. zero-length vector) if no
      address for this class (e.g. PPP). */
   u8 *hw_address;
@@ -828,6 +831,14 @@
 
 typedef struct
 {
+  u32 *reusable_buffers;
+  u32 padding[14];
+} vnet_interface_per_thread_data_t;
+
+
+
+typedef struct
+{
   /* Hardware interfaces. */
   vnet_hw_interface_t *hw_interfaces;
 
@@ -864,6 +875,9 @@
   u32 pcap_pkts_to_capture;
   uword *pcap_drop_filter_hash;
 
+  /* per-thread data */
+  vnet_interface_per_thread_data_t *per_thread_data;
+
   /* feature_arc_index */
   u8 output_feature_arc_index;
 } vnet_interface_main_t;
diff --git a/src/vnet/interface_funcs.h b/src/vnet/interface_funcs.h
index 557ed14..20c9c44 100644
--- a/src/vnet/interface_funcs.h
+++ b/src/vnet/interface_funcs.h
@@ -429,6 +429,7 @@
 {
   VNET_INTERFACE_OUTPUT_ERROR_INTERFACE_DOWN,
   VNET_INTERFACE_OUTPUT_ERROR_INTERFACE_DELETED,
+  VNET_INTERFACE_OUTPUT_ERROR_NO_BUFFERS_FOR_GSO,
 } vnet_interface_output_error_t;
 
 /* Format for interface output traces. */
diff --git a/src/vnet/interface_output.c b/src/vnet/interface_output.c
index 89ce274..903378b 100644
--- a/src/vnet/interface_output.c
+++ b/src/vnet/interface_output.c
@@ -47,6 +47,7 @@
 typedef struct
 {
   u32 sw_if_index;
+  u32 flags;
   u8 data[128 - sizeof (u32)];
 }
 interface_output_trace_t;
@@ -69,23 +70,32 @@
 	  (vnm->interface_main.sw_interfaces, t->sw_if_index))
 	{
 	  /* the interface may have been deleted by the time the trace is printed */
-	  s = format (s, "sw_if_index: %d\n%U%U",
-		      t->sw_if_index,
-		      format_white_space, indent,
-		      node->format_buffer ? node->
-		      format_buffer : format_hex_bytes, t->data,
-		      sizeof (t->data));
+	  s = format (s, "sw_if_index: %d ", t->sw_if_index);
+#define _(bit, name, v) \
+          if (v && (t->flags & VNET_BUFFER_F_##name)) \
+            s = format (s, "%s ", v);
+	  foreach_vnet_buffer_flag
+#undef _
+	    s = format (s, "\n%U%U",
+			format_white_space, indent,
+			node->format_buffer ? node->format_buffer :
+			format_hex_bytes, t->data, sizeof (t->data));
 	}
       else
 	{
 	  si = vnet_get_sw_interface (vnm, t->sw_if_index);
-
-	  s = format (s, "%U\n%U%U",
-		      format_vnet_sw_interface_name, vnm, si,
-		      format_white_space, indent,
-		      node->format_buffer ? node->
-		      format_buffer : format_hex_bytes, t->data,
-		      sizeof (t->data));
+	  s =
+	    format (s, "%U ", format_vnet_sw_interface_name, vnm, si,
+		    t->flags);
+#define _(bit, name, v) \
+          if (v && (t->flags & VNET_BUFFER_F_##name)) \
+            s = format (s, "%s ", v);
+	  foreach_vnet_buffer_flag
+#undef _
+	    s = format (s, "\n%U%U",
+			format_white_space, indent,
+			node->format_buffer ? node->format_buffer :
+			format_hex_bytes, t->data, sizeof (t->data));
 	}
     }
   return s;
@@ -121,6 +131,7 @@
 	{
 	  t0 = vlib_add_trace (vm, node, b0, sizeof (t0[0]));
 	  t0->sw_if_index = vnet_buffer (b0)->sw_if_index[VLIB_TX];
+	  t0->flags = b0->flags;
 	  clib_memcpy (t0->data, vlib_buffer_get_current (b0),
 		       sizeof (t0->data));
 	}
@@ -128,6 +139,7 @@
 	{
 	  t1 = vlib_add_trace (vm, node, b1, sizeof (t1[0]));
 	  t1->sw_if_index = vnet_buffer (b1)->sw_if_index[VLIB_TX];
+	  t1->flags = b1->flags;
 	  clib_memcpy (t1->data, vlib_buffer_get_current (b1),
 		       sizeof (t1->data));
 	}
@@ -149,6 +161,7 @@
 	{
 	  t0 = vlib_add_trace (vm, node, b0, sizeof (t0[0]));
 	  t0->sw_if_index = vnet_buffer (b0)->sw_if_index[VLIB_TX];
+	  t0->flags = b0->flags;
 	  clib_memcpy (t0->data, vlib_buffer_get_current (b0),
 		       sizeof (t0->data));
 	}
@@ -199,12 +212,350 @@
   b->flags &= ~VNET_BUFFER_F_OFFLOAD_IP_CKSUM;
 }
 
+static_always_inline int
+ensure_available_buffers (vlib_main_t * vm,
+			  vnet_interface_per_thread_data_t * ptd,
+			  vlib_buffer_t * b0)
+{
+  u32 n_bytes_b0 = vlib_buffer_length_in_chain (vm, b0);
+  u16 max_needed_buffers = n_bytes_b0 / vnet_buffer2 (b0)->gso_size;
+  if (vec_len (ptd->reusable_buffers) < max_needed_buffers)
+    {
+      u32 *bufs;
+      /* grab 2x more so we don't have to ask too often */
+      u16 n_alloc, n_bufs = 2 * max_needed_buffers;
+
+      vec_add2 (ptd->reusable_buffers, bufs, n_bufs);
+      n_alloc = vlib_buffer_alloc (vm, bufs, n_bufs);
+      if (n_alloc < n_bufs)
+	{
+	  return 0;
+	}
+    }
+  return 1;
+}
+
+/*
+ * Initialize the fresh buffer from the "template" buffer,
+ * copy the identical L2/L3/L4 header, adjust the fields
+ * as necessary.
+ */
+static_always_inline void
+init_buffer_from_template (vlib_buffer_t * nb0, vlib_buffer_t * b0,
+			   u16 l234_sz, u8 ** p_dst_ptr, u16 * p_dst_left,
+			   int is_tcp, u32 next_tcp_seq, u32 flags)
+{
+  u16 gso_size = vnet_buffer2 (b0)->gso_size;
+  *p_dst_left = clib_min (gso_size, VLIB_BUFFER_DATA_SIZE - l234_sz);
+  *p_dst_ptr = nb0->data + l234_sz;
+
+  nb0->current_data = 0;
+  nb0->total_length_not_including_first_buffer = 0;
+
+  nb0->flags = VLIB_BUFFER_TOTAL_LENGTH_VALID | flags;
+  vnet_buffer (nb0)->sw_if_index[VLIB_RX] =
+    vnet_buffer (b0)->sw_if_index[VLIB_RX];
+  vnet_buffer (nb0)->sw_if_index[VLIB_TX] =
+    vnet_buffer (b0)->sw_if_index[VLIB_TX];
+
+  vnet_buffer (nb0)->l2_hdr_offset = vnet_buffer (b0)->l2_hdr_offset;
+  vnet_buffer (nb0)->l3_hdr_offset = vnet_buffer (b0)->l3_hdr_offset;
+  vnet_buffer (nb0)->l4_hdr_offset = vnet_buffer (b0)->l4_hdr_offset;
+  clib_memcpy (nb0->data, b0->data, l234_sz);
+  nb0->current_length = l234_sz;
+
+  /* if TCP - adjust the sequence number */
+  if (is_tcp)
+    {
+      tcp_header_t *tcp =
+	(tcp_header_t *) (nb0->data + vnet_buffer (nb0)->l4_hdr_offset);
+      tcp->seq_number = clib_host_to_net_u32 (next_tcp_seq);
+    }
+}
+
+static_always_inline void
+wipe_buffer (vlib_buffer_t * nb0)
+{
+  /* clean up the buffer before reuse */
+  nb0->next_buffer = 0;
+  nb0->flags = 0;
+}
+
+static_always_inline void
+drop_one_buffer_and_count (vlib_main_t * vm, vnet_main_t * vnm,
+			   vlib_node_runtime_t * node, u32 * pbi0,
+			   u32 drop_error_code)
+{
+  u32 thread_index = vm->thread_index;
+  vnet_interface_output_runtime_t *rt = (void *) node->runtime_data;
+
+  vlib_simple_counter_main_t *cm;
+  cm =
+    vec_elt_at_index (vnm->interface_main.sw_if_counters,
+		      VNET_INTERFACE_COUNTER_TX_ERROR);
+  vlib_increment_simple_counter (cm, thread_index, rt->sw_if_index, 1);
+
+  vlib_error_drop_buffers (vm, node, pbi0,
+			   /* buffer stride */ 1,
+			   /* n_buffers */ 1,
+			   VNET_INTERFACE_OUTPUT_NEXT_DROP,
+			   node->node_index, drop_error_code);
+}
+
+/**
+ * A helper function to segment the GSO buffer.
+ *
+ * The first buffer is fixed up in-place in b0,
+ * the remaining ones are prepared in
+ * ptd->reusable_buffers[0..N]
+ *
+ * The caller must have ensured there is enough
+ * of them available.
+ *
+ * The source buffers initially chained via b0->next_buffer
+ * from which we are done copying data, are detached from b0
+ * and appended to ptd->reusable_buffers.
+ *
+ * Return the number of the buffers the caller
+ * needs to send from ptd->reusable_buffers.
+ *
+ * The caller also must of course delete them from there.
+ */
+
+static_always_inline u16
+segment_gso_buffer (vlib_main_t * vm, vnet_interface_per_thread_data_t * ptd,
+		    int do_tx_offloads, u32 bi0, vlib_buffer_t * b0,
+		    u32 n_bytes_b0)
+{
+  u16 n_tx_bufs = 0;
+  int is_ip4 = b0->flags & VNET_BUFFER_F_IS_IP4;
+  int is_ip6 = b0->flags & VNET_BUFFER_F_IS_IP6;
+  ASSERT (b0->flags & VNET_BUFFER_F_L2_HDR_OFFSET_VALID);
+  ASSERT (b0->flags & VNET_BUFFER_F_L3_HDR_OFFSET_VALID);
+  ip4_header_t *ip4 =
+    (ip4_header_t *) (b0->data + vnet_buffer (b0)->l3_hdr_offset);
+  ip6_header_t *ip6 =
+    (ip6_header_t *) (b0->data + vnet_buffer (b0)->l3_hdr_offset);
+  ASSERT (is_ip4 || is_ip6);
+  u16 gso_size = vnet_buffer2 (b0)->gso_size;
+  /* after segmentation it will be a first of series of regular packets, so clear the flag */
+  b0->flags &= ~VNET_BUFFER_F_GSO;
+  b0->flags |= VNET_BUFFER_F_OFFLOAD_IP_CKSUM;
+
+  b0->total_length_not_including_first_buffer = 0;
+  b0->flags |= VLIB_BUFFER_TOTAL_LENGTH_VALID;
+
+  if (!(b0->flags & VNET_BUFFER_F_L4_HDR_OFFSET_VALID))
+    {
+      if (is_ip4)
+	{
+	  vnet_buffer (b0)->l4_hdr_offset =
+	    vnet_buffer (b0)->l3_hdr_offset + ip4_header_bytes (ip4);
+	}
+      else
+	{
+	  vnet_buffer (b0)->l4_hdr_offset = vnet_buffer (b0)->l3_hdr_offset + sizeof (ip6_header_t);	/* FIXME IPv6 EH traversal */
+	}
+      b0->flags |= VNET_BUFFER_F_L4_HDR_OFFSET_VALID;
+    }
+  int l4_proto = -1;
+  int l4_hdr_sz = 0;
+  int is_tcp = 0;
+  int is_udp = 0;
+  u8 save_tcp_flags = 0;
+  u32 next_tcp_seq = 0;
+  tcp_header_t *tcp =
+    (tcp_header_t *) (b0->data + vnet_buffer (b0)->l4_hdr_offset);
+  udp_header_t *udp =
+    (udp_header_t *) (b0->data + vnet_buffer (b0)->l4_hdr_offset);
+  if (is_ip4)
+    {
+      l4_proto = ip4->protocol;
+    }
+  else
+    {
+      l4_proto = ip6->protocol;	/* FIXME: IPv6 EH traversal */
+    }
+  if (l4_proto == IP_PROTOCOL_UDP)
+    {
+      is_udp = 1;
+      b0->flags |= VNET_BUFFER_F_OFFLOAD_UDP_CKSUM;
+      l4_hdr_sz = sizeof (*udp);
+    }
+  else if (l4_proto == IP_PROTOCOL_TCP)
+    {
+      is_tcp = 1;
+      b0->flags |= VNET_BUFFER_F_OFFLOAD_TCP_CKSUM;
+      l4_hdr_sz = tcp_header_bytes (tcp);
+      next_tcp_seq = clib_net_to_host_u32 (tcp->seq_number);
+      /* store original flags for last packet and reset FIN and PSH */
+      save_tcp_flags = tcp->flags;
+      tcp->checksum = 0;
+    }
+  u32 default_flags = b0->flags & ~VLIB_BUFFER_NEXT_PRESENT;
+  u16 l234_sz = vnet_buffer (b0)->l4_hdr_offset + l4_hdr_sz;
+  int first_data_size = clib_min (gso_size, b0->current_length - l234_sz);
+  next_tcp_seq += first_data_size;
+  /*
+     Now truncate the first buffer as needed, set the source and dest
+     ptrs, and resegment the src buffers into dst buffers.
+     Recycle the "empty" src buffers to the end of reusable_buffers vector,
+     so the next GSO packet that needs to be segmented can make use of them.
+   */
+  u8 *src_ptr, *dst_ptr;
+  u16 src_left, dst_left;
+  u32 total_src_left;
+  /* current source buffer */
+  vlib_buffer_t *csb0 = b0;
+  u32 csbi0 = bi0;
+  /* current dest buffer */
+  vlib_buffer_t *cdb0;
+  u16 dbi = 0;
+  total_src_left = n_bytes_b0 - l234_sz - first_data_size;
+  if (total_src_left)
+    {
+      /* Work to do. Set the pointers and truncate the b0 since there will be at least one more */
+      src_ptr = b0->data + l234_sz + first_data_size;
+      src_left = b0->current_length - l234_sz - first_data_size;
+      b0->current_length = l234_sz + first_data_size;
+
+      if (is_tcp)
+	tcp->flags &= ~(TCP_FLAG_FIN | TCP_FLAG_PSH);
+      if (is_udp)
+	udp->length =
+	  clib_host_to_net_u16 (b0->current_length -
+				vnet_buffer (b0)->l4_hdr_offset);
+      if (is_ip4)
+	{
+	  ip4->length =
+	    clib_host_to_net_u16 (b0->current_length -
+				  vnet_buffer (b0)->l3_hdr_offset);
+	}
+      else
+	{
+	  ip6->payload_length =
+	    clib_host_to_net_u16 (b0->current_length -
+				  vnet_buffer (b0)->l4_hdr_offset);
+	}
+
+
+      /* grab a second buffer and prepare the loop */
+      cdb0 = vlib_get_buffer (vm, ptd->reusable_buffers[dbi++]);
+      init_buffer_from_template (cdb0, b0, l234_sz, &dst_ptr,
+				 &dst_left, is_tcp,
+				 next_tcp_seq, default_flags);
+      /* an arbitrary large number to catch the runaway loops */
+      int nloops = 2000;
+      while (total_src_left)
+	{
+	  ASSERT (nloops-- > 0);
+	  u16 bytes_to_copy = clib_min (src_left, dst_left);
+
+	  clib_memcpy (dst_ptr, src_ptr, bytes_to_copy);
+
+	  src_left -= bytes_to_copy;
+	  src_ptr += bytes_to_copy;
+	  total_src_left -= bytes_to_copy;
+	  dst_left -= bytes_to_copy;
+	  dst_ptr += bytes_to_copy;
+	  next_tcp_seq += bytes_to_copy;
+	  cdb0->current_length += bytes_to_copy;
+
+	  if (0 == src_left)
+	    {
+	      int has_next = (csb0->flags & VLIB_BUFFER_NEXT_PRESENT);
+	      u32 next_bi = csb0->next_buffer;
+	      csb0->flags &= ~VLIB_BUFFER_NEXT_PRESENT;
+	      csb0->next_buffer = ~0;
+
+	      /* anything other than the first src buffer is thrown into recycling */
+	      if (csb0 != b0)
+		{
+		  vec_add1 (ptd->reusable_buffers, csbi0);
+		  wipe_buffer (csb0);
+		}
+
+	      /* init src to the next buffer in chain */
+	      if (has_next)
+		{
+		  csbi0 = next_bi;
+		  csb0 = vlib_get_buffer (vm, csbi0);
+		  src_left = csb0->current_length;
+		  src_ptr = csb0->data;
+		}
+	      if (!has_next)
+		{
+		  if (total_src_left > 0)
+		    {
+		      clib_warning
+			("bug: runaway loop while segmenting GSO: total_src_left: %d",
+			 total_src_left);
+		    }
+		  break;
+		}
+	    }
+	  if (0 == dst_left && total_src_left)
+	    {
+	      if (do_tx_offloads)
+		calc_checksums (vm, cdb0);
+	      cdb0 = vlib_get_buffer (vm, ptd->reusable_buffers[dbi++]);
+	      init_buffer_from_template (cdb0, b0, l234_sz,
+					 &dst_ptr, &dst_left,
+					 is_tcp, next_tcp_seq, default_flags);
+	    }
+	}
+      if (is_tcp)
+	{
+	  // Restore the TCP flags for the last segment
+	  tcp_header_t *tcpX =
+	    (tcp_header_t *) (cdb0->data + vnet_buffer (cdb0)->l4_hdr_offset);
+	  tcpX->flags = save_tcp_flags;
+	}
+      if (is_udp)
+	{
+	  udp_header_t *udpX =
+	    (udp_header_t *) (cdb0->data + vnet_buffer (cdb0)->l4_hdr_offset);
+	  udpX->length =
+	    clib_host_to_net_u16 (cdb0->current_length -
+				  vnet_buffer (cdb0)->l4_hdr_offset);
+	}
+      if (is_ip4)
+	{
+	  ip4_header_t *ip4X =
+	    (ip4_header_t *) (cdb0->data + vnet_buffer (cdb0)->l3_hdr_offset);
+	  ip4X->length =
+	    clib_host_to_net_u16 (cdb0->current_length -
+				  vnet_buffer (cdb0)->l3_hdr_offset);
+	}
+      else
+	{
+	  ip6_header_t *ip6X =
+	    (ip6_header_t *) (cdb0->data + vnet_buffer (cdb0)->l3_hdr_offset);
+	  ip6X->payload_length =
+	    clib_host_to_net_u16 (cdb0->current_length -
+				  vnet_buffer (cdb0)->l4_hdr_offset);
+	}
+      if (do_tx_offloads)
+	calc_checksums (vm, cdb0);
+
+      /* transmit the segmented buffers */
+      n_tx_bufs = dbi;
+    }
+  else
+    {
+      /* a small GSO packet - nothing to do. */
+    }
+  return n_tx_bufs;
+}
+
+
+
 static_always_inline uword
 vnet_interface_output_node_inline (vlib_main_t * vm,
 				   vlib_node_runtime_t * node,
 				   vlib_frame_t * frame, vnet_main_t * vnm,
 				   vnet_hw_interface_t * hi,
-				   int do_tx_offloads)
+				   int do_tx_offloads, int do_segmentation)
 {
   vnet_interface_output_runtime_t *rt = (void *) node->runtime_data;
   vnet_sw_interface_t *si;
@@ -216,6 +567,8 @@
   u32 next_index = VNET_INTERFACE_OUTPUT_NEXT_TX;
   u32 current_config_index = ~0;
   u8 arc = im->output_feature_arc_index;
+  vnet_interface_per_thread_data_t *ptd =
+    vec_elt_at_index (im->per_thread_data, thread_index);
 
   n_buffers = frame->n_vectors;
 
@@ -297,14 +650,24 @@
 	  to_tx[1] = bi1;
 	  to_tx[2] = bi2;
 	  to_tx[3] = bi3;
-	  from += 4;
-	  to_tx += 4;
-	  n_left_to_tx -= 4;
 
 	  b0 = vlib_get_buffer (vm, bi0);
 	  b1 = vlib_get_buffer (vm, bi1);
 	  b2 = vlib_get_buffer (vm, bi2);
 	  b3 = vlib_get_buffer (vm, bi3);
+
+	  or_flags = b0->flags | b1->flags | b2->flags | b3->flags;
+	  if (do_segmentation)
+	    {
+	      if (or_flags & VNET_BUFFER_F_GSO)
+		{
+		  break;	/* go to single loop if we need GSO segmentation */
+		}
+	    }
+
+	  from += 4;
+	  to_tx += 4;
+	  n_left_to_tx -= 4;
 
 	  /* Be grumpy about zero length buffers for benefit of
 	     driver tx function. */
@@ -373,7 +736,6 @@
 					       n_bytes_b3);
 	    }
 
-	  or_flags = b0->flags | b1->flags | b2->flags | b3->flags;
 
 	  if (do_tx_offloads)
 	    {
@@ -419,6 +781,80 @@
 	      b0->current_config_index = current_config_index;
 	    }
 
+	  if (do_segmentation)
+	    {
+	      /*
+
+	         // test code
+
+	         if (n_bytes_b0 > 1500) {
+	         b0->flags |= VNET_BUFFER_F_GSO;
+	         b0->flags |= VNET_BUFFER_F_IS_IP4;
+	         vnet_buffer2 (b0)->gso_size = 1448;
+	         }
+	       */
+	      if (b0->flags & VNET_BUFFER_F_GSO)
+		{
+
+		  /* We don't wanna deal with the hassles of allocation in-process */
+		  if (PREDICT_FALSE (!ensure_available_buffers (vm, ptd, b0)))
+		    {
+		      /*
+		       * We could not allocate enough buffers...
+		       * Undo the enqueue of the first buffer, drop it and increment the counter.
+		       * Having "from -= 1;" would have meant an infinite loop.
+		       */
+		      to_tx -= 1;
+		      n_left_to_tx += 1;
+		      drop_one_buffer_and_count (vm, vnm, node, from - 1,
+						 VNET_INTERFACE_OUTPUT_ERROR_NO_BUFFERS_FOR_GSO);
+
+		      continue;
+		    }
+		  u16 n_segmented_bufs =
+		    segment_gso_buffer (vm, ptd, do_tx_offloads, bi0, b0,
+					n_bytes_b0);
+		  u16 n_tx_bufs = n_segmented_bufs;
+
+		  u32 *gso_tx = ptd->reusable_buffers;
+		  while (n_tx_bufs > 0)
+		    {
+		      if (n_tx_bufs >= n_left_to_tx)
+			{
+			  while (n_left_to_tx > 0)
+			    {
+			      to_tx[0] = gso_tx[0];
+			      to_tx += 1;
+			      gso_tx += 1;
+			      n_left_to_tx -= 1;
+			      n_tx_bufs -= 1;
+			      n_packets += 1;
+			    }
+			  vlib_put_next_frame (vm, node, next_index,
+					       n_left_to_tx);
+			  vlib_get_new_next_frame (vm, node, next_index,
+						   to_tx, n_left_to_tx);
+			}
+		      else
+			{
+			  while (n_tx_bufs > 0)
+			    {
+			      to_tx[0] = gso_tx[0];
+			      to_tx += 1;
+			      gso_tx += 1;
+			      n_left_to_tx -= 1;
+			      n_tx_bufs -= 1;
+			      n_packets += 1;
+			    }
+			}
+		    }
+		  vec_delete (ptd->reusable_buffers, n_segmented_bufs, 0);
+		}
+	    }
+
+	  if (do_tx_offloads)
+	    calc_checksums (vm, b0);
+
 	  if (PREDICT_FALSE (tx_swif0 != rt->sw_if_index))
 	    {
 
@@ -427,11 +863,7 @@
 					       thread_index, tx_swif0, 1,
 					       n_bytes_b0);
 	    }
-
-	  if (do_tx_offloads)
-	    calc_checksums (vm, b0);
 	}
-
       vlib_put_next_frame (vm, node, next_index, n_left_to_tx);
     }
 
@@ -453,11 +885,27 @@
   hi = vnet_get_sup_hw_interface (vnm, rt->sw_if_index);
 
   if (hi->flags & VNET_HW_INTERFACE_FLAG_SUPPORTS_TX_L4_CKSUM_OFFLOAD)
-    return vnet_interface_output_node_inline (vm, node, frame, vnm, hi,
-					      /* do_tx_offloads */ 0);
+    {
+      if (hi->flags & VNET_HW_INTERFACE_FLAG_SUPPORTS_GSO)
+	return vnet_interface_output_node_inline (vm, node, frame, vnm, hi,
+						  /* do_tx_offloads */ 0,
+						  /* do_segmentation */ 0);
+      else
+	return vnet_interface_output_node_inline (vm, node, frame, vnm, hi,
+						  /* do_tx_offloads */ 0,
+						  /* do_segmentation */ 1);
+    }
   else
-    return vnet_interface_output_node_inline (vm, node, frame, vnm, hi,
-					      /* do_tx_offloads */ 1);
+    {
+      if (hi->flags & VNET_HW_INTERFACE_FLAG_SUPPORTS_GSO)
+	return vnet_interface_output_node_inline (vm, node, frame, vnm, hi,
+						  /* do_tx_offloads */ 1,
+						  /* do_segmentation */ 0);
+      else
+	return vnet_interface_output_node_inline (vm, node, frame, vnm, hi,
+						  /* do_tx_offloads */ 1,
+						  /* do_segmentation */ 1);
+    }
 }
 
 VLIB_NODE_FUNCTION_MULTIARCH_CLONE (vnet_interface_output_node);
diff --git a/src/vnet/ip/ip4_forward.c b/src/vnet/ip/ip4_forward.c
index 23e90df..3329e3b 100644
--- a/src/vnet/ip/ip4_forward.c
+++ b/src/vnet/ip/ip4_forward.c
@@ -53,6 +53,7 @@
 #include <vnet/dpo/load_balance_map.h>
 #include <vnet/dpo/classify_dpo.h>
 #include <vnet/mfib/mfib_table.h>	/* for mFIB table and entry creation */
+#include <vnet/buffer.h>
 
 #include <vnet/ip/ip4_forward.h>
 
@@ -2264,12 +2265,19 @@
       CLIB_PREFETCH (p, CLIB_CACHE_LINE_BYTES, LOAD);
 
       /* Check MTU of outgoing interface. */
-      ip4_mtu_check (b[0], clib_net_to_host_u16 (ip0->length),
+      u16 ip0_len =
+	(b[0]->flags & VNET_BUFFER_F_GSO) ? vnet_buffer2 (b[0])->gso_size +
+	ip4_header_bytes (ip0) : clib_net_to_host_u16 (ip0->length);
+      u16 ip1_len =
+	(b[1]->flags & VNET_BUFFER_F_GSO) ? vnet_buffer2 (b[1])->gso_size +
+	ip4_header_bytes (ip1) : clib_net_to_host_u16 (ip1->length);
+
+      ip4_mtu_check (b[0], ip0_len,
 		     adj0[0].rewrite_header.max_l3_packet_bytes,
 		     ip0->flags_and_fragment_offset &
 		     clib_host_to_net_u16 (IP4_HEADER_FLAG_DONT_FRAGMENT),
 		     next + 0, &error0);
-      ip4_mtu_check (b[1], clib_net_to_host_u16 (ip1->length),
+      ip4_mtu_check (b[1], ip1_len,
 		     adj1[0].rewrite_header.max_l3_packet_bytes,
 		     ip1->flags_and_fragment_offset &
 		     clib_host_to_net_u16 (IP4_HEADER_FLAG_DONT_FRAGMENT),
@@ -2392,7 +2400,10 @@
       vnet_buffer (b[0])->ip.save_rewrite_length = rw_len0;
 
       /* Check MTU of outgoing interface. */
-      ip4_mtu_check (b[0], clib_net_to_host_u16 (ip0->length),
+      u16 ip0_len =
+	(b[0]->flags & VNET_BUFFER_F_GSO) ? vnet_buffer2 (b[0])->gso_size +
+	ip4_header_bytes (ip0) : clib_net_to_host_u16 (ip0->length);
+      ip4_mtu_check (b[0], ip0_len,
 		     adj0[0].rewrite_header.max_l3_packet_bytes,
 		     ip0->flags_and_fragment_offset &
 		     clib_host_to_net_u16 (IP4_HEADER_FLAG_DONT_FRAGMENT),
diff --git a/src/vnet/ip/ip6_forward.c b/src/vnet/ip/ip6_forward.c
index 9a9a64b..6204347 100644
--- a/src/vnet/ip/ip6_forward.c
+++ b/src/vnet/ip/ip6_forward.c
@@ -1732,12 +1732,19 @@
 	    }
 
 	  /* Check MTU of outgoing interface. */
-	  ip6_mtu_check (p0, clib_net_to_host_u16 (ip0->payload_length) +
-			 sizeof (ip6_header_t),
+	  u16 ip0_len =
+	    (p0->flags & VNET_BUFFER_F_GSO) ? vnet_buffer2 (p0)->gso_size +
+	    sizeof (ip6_header_t) : clib_net_to_host_u16 (ip0->payload_length)
+	    + sizeof (ip6_header_t);
+	  u16 ip1_len =
+	    (p1->flags & VNET_BUFFER_F_GSO) ? vnet_buffer2 (p1)->gso_size +
+	    sizeof (ip6_header_t) : clib_net_to_host_u16 (ip1->payload_length)
+	    + sizeof (ip6_header_t);
+
+	  ip6_mtu_check (p0, ip0_len,
 			 adj0[0].rewrite_header.max_l3_packet_bytes,
 			 is_locally_originated0, &next0, &error0);
-	  ip6_mtu_check (p1, clib_net_to_host_u16 (ip1->payload_length) +
-			 sizeof (ip6_header_t),
+	  ip6_mtu_check (p1, ip1_len,
 			 adj1[0].rewrite_header.max_l3_packet_bytes,
 			 is_locally_originated1, &next1, &error1);
 
@@ -1876,8 +1883,11 @@
 	    }
 
 	  /* Check MTU of outgoing interface. */
-	  ip6_mtu_check (p0, clib_net_to_host_u16 (ip0->payload_length) +
-			 sizeof (ip6_header_t),
+	  u16 ip0_len =
+	    (p0->flags & VNET_BUFFER_F_GSO) ? vnet_buffer2 (p0)->gso_size +
+	    sizeof (ip6_header_t) : clib_net_to_host_u16 (ip0->payload_length)
+	    + sizeof (ip6_header_t);
+	  ip6_mtu_check (p0, ip0_len,
 			 adj0[0].rewrite_header.max_l3_packet_bytes,
 			 is_locally_originated0, &next0, &error0);
 
